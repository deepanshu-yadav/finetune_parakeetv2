For tokenising the input data on Windows
python NeMO\scripts\tokenizers\process_asr_text_tokenizer.py --manifest=train_manifest.json --data_root=tokenizer_output --vocab_size=1024 --tokenizer=spe --spe_type=bpe --spe_character_coverage=1.0 --no_lower_case --log

For tokeninsing the data on Linux
python NeMO/scripts/tokenizers/process_asr_text_tokenizer.py --manifest=train_manifest.json --data_root=tokenizer_output --vocab_size=1024 --tokenizer=spe --spe_type=bpe --spe_character_coverage=1.0 --no_lower_case --log
